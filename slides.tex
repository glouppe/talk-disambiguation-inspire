\documentclass{beamer}
\usepackage{etex}

\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{minted}
\usepackage{natbib}
\usepackage{eulervm}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{tikz}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usetikzlibrary{shapes,arrows}
\tikzset{
  every overlay node/.style={
    draw=white,fill=white,anchor=north west,
  },
  every overlay node border/.style={
    draw=black,fill=white,anchor=north west,rounded corners,
  },
}
% Usage:
% \tikzoverlay at (-1cm,-5cm) {content};
% or
% \tikzoverlay[text width=5cm] at (-1cm,-5cm) {content};
\def\tikzoverlay{%
   \tikz[baseline,overlay]\node[every overlay node]
}%
\def\tikzoverlayborder{%
   \tikz[baseline,overlay]\node[every overlay node border]
}%

\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

\title{\bf Machine Learning for Author disambiguation}
\author{Gilles Louppe}
\institute{CERN}
\date{March 2, 2015}

\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\begin{frame}
\titlepage
\end{frame}


% Motivation ==================================================================

\begin{frame}{Motivation}
For each author, group together all his publications, and only those.

\footnotesize{
\begin{columns}[T]

\begin{column}{0.33\textwidth}
\begin{figure}
\texttt{M.S.Smith.1}
\includegraphics[width=\textwidth]{figures/no-more.png}
\end{figure}
\end{column}
\begin{column}{0.33\textwidth}
\begin{figure}
\texttt{Z.Liang.4}
\includegraphics[width=\textwidth]{figures/no-less.png}

\texttt{Z.Liang.5}
\includegraphics[width=\textwidth]{figures/no-less.png}

...

\texttt{Z.Liang.83}
\includegraphics[width=\textwidth]{figures/no-less.png}
\end{figure}
\end{column}
\begin{column}{0.33\textwidth}
\begin{figure}
\texttt{S.W.Hawking.1}
\includegraphics[width=\textwidth]{figures/exact.png}
\end{figure}
\end{column}

\end{columns}

\begin{columns}[T]
\begin{column}{0.33\textwidth}
\begin{center}{\it No more}\end{center}
\end{column}
\begin{column}{0.33\textwidth}
\begin{center}{\it No less}\end{center}
\end{column}
\begin{column}{0.33\textwidth}
\begin{center}{\it But all and only the correct ones}\end{center}
\end{column}
\end{columns}}
\end{frame}

\begin{frame}

\tikzstyle{block} = [rectangle, draw, text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = []

\begin{center}
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (clustering) {?};
    \node [cloud, above of=clustering] (input1) {$\{s_1, s_2, ..., s_N\}$};
    \node [cloud, below of=clustering] (output) {$\{ \underbrace{\{ s_1, s_3, s_4 \}}_{\text{author 1}}, \underbrace{\{ s_2, s_5\}}_{\text{author 2}}, ... \}$};
    % Draw edges
    \path [line] (input1) -- (clustering);
    \path [line] (clustering) -- (output);
\end{tikzpicture}
\end{center}
\end{frame}



% Spread of the problem =======================================================

\begin{frame}{Spread of the problem}

As extracted from claimed publications in INSPIRE,

\begin{itemize}
\item {\color{blue} Authors have on average $2.06$ name variants (synonyms)}\\
Eg.: Doe, John; Doe, J.
\item {\color{blue} Unique name variants are shared on average by $1.04$ authors (homonyms)}
\end{itemize}

Clustering on exact full names or last name + first initial, should
yield very good results on average.

\vskip1em

{\color{red} But}, disambiguation issues are expected to amplify with the rise of Asian researchers:
Caucasian names (now representative of INSPIRE authors) are almost never
ambiguous, while Asian names are very often.

% compute average number of name variants per author (synonyms)
% compute average number of authors per name variants
% => easy but we want to do better

\end{frame}


% How would you fare? =========================================================

\begin{frame}{How would \textit{you} fare?}

\only<1-2>{
\begin{figure}
\includegraphics[scale=0.35]{figures/author1/1.png}\\[2em]
\includegraphics[scale=0.35]{figures/author1/2.png}\\[2em]
\uncover<2>{\color{blue} \cmark~ Same authors}
\end{figure}
}
\only<3-4>{
\begin{figure}
\includegraphics[scale=0.25]{figures/author2/3.png}\\[2em]
\includegraphics[scale=0.35]{figures/author2/1.png}\\[1em]
\uncover<4>{\color{blue} \cmark~ Same authors}
\end{figure}
}
% \only<5-6>{
% \begin{figure}
% \includegraphics[scale=0.35]{figures/author1/5.png}\\[2em]
% \includegraphics[scale=0.35]{figures/author1/4.png}\\[2em]
% \uncover<6>{\color{blue} \cmark~ Same authors}
% \end{figure}
% }
\only<5-6>{
\begin{figure}
\includegraphics[scale=0.35]{figures/author1/6.png}\\[2em]
\hspace*{-2cm}\includegraphics[scale=0.35]{figures/author2/2.png}\\[2em]
\uncover<6>{\color{red} \xmark~ Different authors}
\end{figure}
}
\only<7-8>{
\begin{figure}
\includegraphics[scale=0.35]{figures/author1/3.png}\\[2em]
\includegraphics[scale=0.35]{figures/author1/4.png}\\[2em]
\uncover<8>{\color{blue} \cmark~ Same authors}
\end{figure}
}

\end{frame}


% Learning ====================================================================

\begin{frame}{Learning from data}

\begin{itemize}
\item Manual disambiguation is {\color{red} long and difficult}, even for experienced curators.\\[2em]
\item Couldn't we {\color{blue} automatically find a set of rules} to disambiguate two signatures?

$$\varphi(s_1, s_2) = \begin{cases}
    0 & \text{if $s_1$ and $s_2$ belong to the same author},\\
    1 & \text{otherwise}.
  \end{cases}$$

\item This is a machine learning task called {\color{red}supervised learning}.
\end{itemize}

\end{frame}


% Supervised learning =========================================================

\begin{frame}{Supervised learning}

\begin{itemize}
\item The \X{inputs} are random variables \X{$X = X_1$, ..., $X_p$};
\item The \y{output} is a random variable \y{$Y$}.
\end{itemize}

\begin{itemize}
\item Data comes as a finite learning set $${\cal L} = \{(\X{\mathbf{x}_i}, \y{y_i}) | i = 0, \dots, N-1 \},$$
where \X{$\mathbf{x}_i \in {\cal X} = {\cal X}_1 \times ... \times {\cal X}_p$} and \y{$y_i \in {\cal Y}$}
are randomly drawn from $P_{\X{X},\y{Y}}$.\\
\vspace{0.3cm}
\small{
E.g.,:\\
$(\X{\mathbf{x}_i}, \y{y_i}) = ((\X{\text{name sim.}=0.7, \X{\text{title sim.}=0.3}, ...)}, \y{\text{same authors}})$ \\
$(\X{\mathbf{x}_j}, \y{y_j}) = ((\X{\text{name sim.}=0.1, \X{\text{title sim.}=0.5}, ...)}, \y{\text{different authors}})$
}
\end{itemize}

\begin{itemize}
\item The goal is to find a model $\model{\varphi_{\cal L}}: \X{{\cal X}} \mapsto \y{{\cal Y}}$ minimizing
$$
Err(\model{\varphi_{\cal L}}) = \mathbb{E}_{\X{X}, \y{Y}}\{ L(\y{Y}, \model{\varphi_{\cal L}}(\X{X})) \}.
$$
\end{itemize}

\end{frame}


% DT ==========================================================================

\begin{frame}{Decision trees [L. Breiman, 1984]}
\begin{columns}
\begin{column}{0.3\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{./figures/tree-partition-d.pdf}
\end{figure}
\end{column}
\begin{column}{0.7\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{./figures/tree-simple.pdf}
\end{figure}
\end{column}
\end{columns}

\vspace{0.5cm}

\begin{itemize}
\item {\color{blue}Heterogeneous} data
\item {\color{blue}Non-parametric} model (detect non-linear interactions)
\item Easily {\color{blue}interpretable}
\item But {\color{red} prone to overfitting} (high variance)
\end{itemize}

\tikzoverlay[text width=10cm] at (-0.2cm, 0.1cm) {
\begin{columns}[c]
\begin{column}{0.2\textwidth}
    \begin{figure}
      \includegraphics[width=2em]{./figures/scikit-learn-logo-thumb.png}
    \end{figure}
\end{column}
\begin{column}{1.0\textwidth}
    \vspace{-0.5cm}\hspace{-0.5cm}{\footnotesize \texttt{sklearn.tree.DecisionTreeClassifier|Regressor}}
\end{column}
\end{columns}
};

% Extensions: {\it Random Forests}, {\it Boosting}, etc.
% \end{column}
% \begin{column}[t]{0.20\textwidth}
%     \begin{figure}
%     \includegraphics[scale=0.35]{./figures/scikit-learn-logo.pdf}
%     \end{figure}
% \end{column}
% \end{columns}

\end{frame}

\begin{frame}{Decision trees}
    \begin{figure}
    \includegraphics[width=\textwidth]{./figures/tree.png}
    \end{figure}
\end{frame}

\begin{frame}{Gradient Boosted Regression Trees [J. Friedman, 1999]}

\begin{itemize}
\item Ensemble of regression trees approximating the (negative) gradient of a loss function
\item Each tree is a successive gradient descent step
\item Low bias and low variance
\end{itemize}


\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./figures/residual_fitting_2.pdf}
\end{figure}

\tikzoverlay[text width=10cm] at (-0.2cm, 0.1cm) {
\begin{columns}[c]
\begin{column}{0.2\textwidth}
    \begin{figure}
      \includegraphics[width=2em]{./figures/scikit-learn-logo-thumb.png}
    \end{figure}
\end{column}
\begin{column}{1.0\textwidth}
    \vspace{-0.5cm}\hspace{-0.5cm}{\footnotesize \texttt{sklearn.ensemble.GradientBoostingClassifier|Regressor}}
\end{column}
\end{columns}
};
\end{frame}

\begin{frame}
\tikzstyle{block} = [rectangle, draw, text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = []

\begin{center}
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (transformer) {Feature extraction};
    \node [cloud, above left of=transformer] (input1) {$s_1$};
    \node [cloud, above right of=transformer] (input2) {$s_2$};
    \node [cloud, below of=transformer] (x) {$\mathbf{x} = (\text{name sim.}=0.7, \text{title sim.}=0.3, ...)$};
    \node [block, below of=x] (gbrt) {GBRT $\model{\varphi}$};
    \node [cloud, below of=gbrt] (output) {$p(\text{$s_1$, $s_2$ have different authors}|\mathbf{x})$};
    % Draw edges
    \path [line] (input1) -- (transformer);
    \path [line] (input2) -- (transformer);
    \path [line] (transformer) -- (x);
    \path [line] (x) -- (gbrt);
    \path [line] (gbrt) -- (output);
\end{tikzpicture}
\end{center}
\end{frame}


% Clustering ==================================================================

\begin{frame}{Disambiguation as a clustering problem}

\begin{itemize}
\item Author disambiguation = {\color{red} clustering signatures that belong to the same author}.\\[1em]

\item Using our model $\model{\varphi}$, the probability
      that two signatures belong to different authors can be used as a (pseudo) distance metric.
\end{itemize}

\tikzstyle{block} = [rectangle, draw, text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = []

\begin{center}
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (clustering) {Hierarchical clustering};
    \node [cloud, above of=clustering] (input1) {$\{s_1, s_2, ..., s_N\}$};
    \node [cloud, left of=clustering] (input2) {$\varphi$};
    \node [cloud, below of=clustering] (output) {$\{ \underbrace{\{ s_1, s_3, s_4 \}}_{\text{author 1}}, \underbrace{\{ s_2, s_5\}}_{\text{author 2}}, ... \}$};
    % Draw edges
    \path [line] (input1) -- (clustering);
    \path [line] (input2) -- (clustering);
    \path [line] (clustering) -- (output);
\end{tikzpicture}
\end{center}

\end{frame}


% Hierarchical clustering =====================================================

\begin{frame}{Hierarchical clustering}

\begin{columns}[T]

\begin{column}{0.33\textwidth}
\includegraphics[scale=0.75]{./figures/hierarchy.png}
\end{column}
\begin{column}{0.66\textwidth}
\begin{itemize}
\item General family of clustering algorithms that {\color{blue}build nested clusters by merging them successively}.\\[2em]

\item This hierarchy of clusters is represented as a tree (or dendrogram).\\[2em]

\item The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.
\end{itemize}
\end{column}

\end{columns}

\end{frame}


% Implementation issues =====================================================

\begin{frame}{Issues}

\begin{itemize}
\item The complexity of hierarchical clustering is {\color{red} $O(N^2)$}. For $N=10^7$ signatures, this is impractical.

{\it Solution:} pre-cluster into blocks all signatures with the same last name + first initial, then cluster each of these blocks.\\[2em]

\item How do you set the {\color{red} cut-off threshold}?

{\it Solution:} using training data (e.g., claimed signatures), pick the threshold that locally maximizes some criterion.
\end{itemize}

\end{frame}


% Results =====================================================================

\begin{frame}{Evaluation}

{\it Protocol:} Use the {\color{blue} claimed signatures} (about 1M) to form {\color{blue} ground truth clusters}. Keep 10\% as a training set to find model parameters, and 90\% as a test set for evaluation.

\begin{align}
\text{$B^3$ Precision} &= \mathbb{E}_s \{ \frac{|\hat{C}(s) \cap C(s)|}{|\hat{C}(s)|}  \} \\
\text{$B^3$ Recall} &= \mathbb{E}_s \{ \frac{|\hat{C}(s) \cap C(s)|}{|C(s)|}  \} \\
\text{$B^3$ F-score} &= \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

where $C(s)$ (resp., $\hat{C}(s)$) is the true (resp., predicted) set of signatures to which $s$ belongs.


% disambiguation is clustering
% how to evaluate clustering?
% give number for simple strategy (all same names baseline 0.5, all same block baseline 1.0)
\end{frame}

\begin{frame}{Results}

\begin{table}
    \centering
    \begin{tabular}{| l c |}
    \hline
        \textit{Method} & \textit{$B^3 \text{F-score}$}  \\
    \hline
    \hline
    Full name & 0.8183 \\
    Last name + First initial & 0.9403 \\
    \hline
    Current prototype & {\color{blue} 0.9701} \\
    \hline
    \end{tabular}
\end{table}

\end{frame}

% Variable importances ========================================================

\begin{frame}{Variable importances}
\begin{figure}
\includegraphics[width=\textwidth]{./figures/imp.png}
\end{figure}
\end{frame}


% Plots =======================================================================

\begin{frame}{Partial dependence plots}
\begin{figure}
\includegraphics[width=0.8\textwidth]{./figures/coauthors.png}\\
\includegraphics[width=0.8\textwidth]{./figures/yeardiff.png}
\end{figure}
\end{frame}

% Improvements ================================================================

\begin{frame}{On-going improvements}

\begin{itemize}
\item Better evaluation metrics.\\[2em]

\item Better exploitation of the training data (e.g., for setting the thresholds, for pre-initializing known clusters, etc).\\[2em]

\item Evaluate alternative input features, supervised learning algorithms and clustering algorithms.\\[2em]

\item Limit model complexity to avoid overfitting and speedup the procedure.\\[2em]

\item Deployment.
\end{itemize}

\end{frame}


\end{document}
